% Created 2019-06-09 Вс 18:46
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{natbib}
\usepackage[linktocpage,pdfstartview=FitH,colorlinks,
linkcolor=blue,anchorcolor=blue,
citecolor=blue,filecolor=blue,menucolor=blue,urlcolor=blue]{hyperref}
\usepackage[utf8]{inputenc} % for cyrilics
\usepackage[russian, english]{babel}
\usepackage[T2A]{fontenc}
\author{Дробин М.Е. (МФТИ ГУ)}
\date{\textit{<2019-04-08 Пн>}}
\title{Предсказание температуры во времени и пространстве}
\hypersetup{
 pdfauthor={Дробин М.Е. (МФТИ ГУ)},
 pdftitle={Предсказание температуры во времени и пространстве},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.2 (Org mode 9.2.3)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

\begin{abstract}
В этой работе исследуется качество прогнозирования температуры для одной из метерологической станций, обучая нейросетевые алгоритмы(RBF, MLP) и
классические(KNN, spatial averaging, inverse distance methods) по данных от других станций в пределах Англии.
\end{abstract}

\section{Введение}
\label{sec:org0197b1e}
Температура воздуха - временные ряды с высоким временным разрешением - измеряют только в немногочисленных, далеко
разнесенных друг от друга метерологических станциях.  Поэтому появляется необходимость в пространственной интерполяции
этих значений на области, где температура. Кроме пространственной интерполяции появляется необходимость в
прогнозировании будущей темпертуры воздуха, т.е. временной интерполяции. Такие задачи появляются в сельском хозяйстве,
прогнозировании погоды в городских условиях и т.д.

В качестве данных используется почасовая информация(влажность, \ldots{}) с 30 метерологических станций Англии(каждая в
отдельном городе) за 2 года. И для некоторых из них предсказывается температура. Данные взяты из сервиса darksky.




\section{Постановка задачи}
\label{sec:org10ee69e}
Даны временные ряды \(temp_{L}(t)\) - температура воздуха, замеренная на
одной из погодных станций в Лондоне, и \(\vec{x_{j}}(t), j \in \{1, 2,
...\}\) - климатические данные городов в окрестности Лондона
(температура, давление, \ldots{}). Необходимо предсказать \(temp_{L}(t + 1)\)

Обобщенная модель для решения этой задачи выглядит следующим образом:
\(temp_{L}(t + 1) = f(temp_{L}(t), \vec{x_{1}}(t), \vec{x_{2}}(t),
...)\). Исследуется зависимость f от близости городов в окрестности
Лондона к самому Лондону. Также исследуется качество известных моделей
машинного обучениня примененых к этим данным. Приводится обоснование
выбора функции потерь для этой задачи, а потому, выводы, построенные
по качеству и некоторым характеристикам примененных моделей зависят не
от моделей, а от самих данных.
\section{Обзор литературы}
\label{sec:org7565eff}
В статье \cite{Spatial-Interpolation-2000-Snell} aвторы сравнили
  качество MLP и spatial average, knn, inverse distance methods для
  задачи интерполяции температуры на 11 NOAA станций, на которых
  измеряют температуру. Эти станции расположены примерно в одном
  штате. Температура с этих станций - это множество ответов алгоритма,
  а данные с GCM(general circulation model) - численное модели - это
  мн-возможным объектов. Такая задача называется downscaling
  GCM. Трудность заключается в том, что пространственное разрешение
  выходных данных GCM - от 2.5° × 2.5° до 8° × 10° в долготу и
  ширину - это слишком грубо для предсказания - поэтому обучают по
  такой сетке нейросеть и предсказывают температуру для точек между
  сетки - выхода GCM. Авторы предсказывали максимальную температуру за
  день T\textsubscript{max} и посчитали качество нейросети(архите- ктура нейросети:
  4-30-11 и 16-54-11 с сигмоидной функцией активацией). Для сетки с
  4мя входами R\textsuperscript{2} и rmse были в среднем по 11 станциям 5.69 и 0.93;
  для сетки с 16ю входами - 5.12 и 0.94.

В статье \cite{nn-prediction-hourly-mean-temp-Tasadduq-2002} авторы
использовали почасовые данные с шести метерологических станций за 2
года с равнины, расположенной в Китае и ограниченной горами. 5 из
них входят в мн-во объектов алгоритмов, для шестой предсказывается
температу.  60\% данных использовалось для обучения, остальное для
теста. Сравнили MLP 5-17-1 с tanh активацией и RBF с скрытым слоем
из 7450 нейронов и гауссовой функцией активацией. Качество на
тестовой выборке: \(R^2\) и rmse(°C) для MLP равны 0.96 и 1.067, а для
RBF - 0.95 и 1.12 соответственно.

  Данные по нескольким городам - погодные данные, взятые из измерителей в аэропортах этих городов.
Задача - предсказать температуру(или прочие погодные показатели, напр., влажность..) для города, не использованного
в обучении,  используя нейросетевые алгоритмы.






\section{Получение данных}
\label{sec:orgc57885d}
Данные были получены через API сервиса \href{https://darksky.net}{darksky} и библиотеки для Python \href{https://pypi.org/project/darkskylib/}{darkskylib}. Для каждого города были скачаны
почасовые данные(иногда с пропусками) с с 1 января 2009 по 1 января 2018 года. Следующие фичи были скачаны:
\begin{itemize}
\item температура воздуха(temperature)
\item скорость ветра (windspeed)
\item процент(от 0 до 1) покрытости неба облаками(cloudCover)
\item относительная влажность, от 0 до 1 (humidity)
\item видимость(visibility) - категориальная переменная, принимает только
\item точка росы в градусах цельсия(dewPoint)
\item краткое описание погоды - категориальная переменная (summary)
\begin{itemize}
\item clear-day
\item clear-night
\item rain
\item snow
\item sleet
\item wind
\item fog
\item cloudy
\item partly-cloudy-day
\item partly-cloudy-night
\end{itemize}
\item температура "по ощущениям" в градусах цельсия (apparentTemperature)
\end{itemize}

\href{https://github.com/gazon1/diplom/blob/master/main.py}{Алгоритм} закачки данных:
for каждый город в списке городов:
    for каждая дата в списке дат с 2009 по 2018 год c периодом в  1 час:
        скачать исторические данные за эту дату для этого города
    добавить данные за этот город в общий датафрейм

\section{EDA}
\label{sec:org63b6129}
\begin{center}
\includegraphics[width=15cm]{./pics/sesonal_decomposition.png}
\end{center}
\begin{center}
\includegraphics[width=15cm]{./pics/rolling_mean.png}
\end{center}

\begin{itemize}
\item Есть ненулевой тренд
\item четко выраженная дневная сезонность
\item годовая сезонность
\end{itemize}
\section{Эксперименты}
\label{sec:orga57a3e2}

\subsection{Схема валидации}
\label{sec:org0b2c5ae}
Модели обучались на первых 80\% данных - до 2016-03-15. Валидировались модели на оставшихся 20\% данных - около 2х
лет. Причина выбора такой схемы валидации проста -  у нас имеется относительно большое кол-во данных(в сравнии с чем?) и
более сложные схемы валидации, например, \href{https://habr.com/ru/company/ods/blog/327242/}{cross-validation on a rolling basis}, оказываются не нужны для построения
устойчивой оценки алгоритма. Более сложные схемы валидации часто применяют, когда данных мало и делить исходную выборку
на 2 невыгодно.

\subsection{Baseline}
\label{sec:orgef46f31}
В качестве алгоритма для сравнения было взято простое предсказание температуры, равное предыдущему значению:
\subsubsection{{\bfseries\sffamily TODO} написать в математической формулировке с формула как я предсказываю в baseline}
\label{sec:orga29cc2b}

Этот алгоритм предсказывает на валидационной выборке с точностью до +/- 2.2 градуса Цельсия

\subsection{MLP}
\label{sec:orge0bdd4e}
Модель обучались на первых 80\% данных - до 2016-03-15. Валидировалась - на оставшихся 20\% данных - около 2х лет.

Данные были нормированы на среднее и дисперсию: \(x_{i} = \frac{x_{i} - \overline{x}}{\sigma}\), где x - это отдельная
фича или таргет (колонка в массиве объекты x фичи) и берется дисперсия и среднее этой фичи и она нормирутеся на свое
среднее и свою дисперсию

фичи: только температуры 3 городов Оксфорд, Кембридж, Брайтон энд Хов, таргет - это Лондон.

\begin{center}
\includegraphics[width=15cm]{./pics/map city predict.png}
\end{center} 

MLP обучался следующим образом: брали данные за 5 дней и температуру Лондона на следующие 24 часа. Оптимизировали mae.
Архитектура нейросети: полносвязный слой с 32 нейронами и relu активацией и полносвязный слой с одним нейроном без
функции активации на выходе. Оптимизатор - RMSprop. Для более быстрой и лучшей сходимости, скорость обучения делилась
на 10, когда функция потерь на валидации увеличивалась или не изменялась:

Генератор данных на керасе для обучения нейросети был заимствован из книги "Deep learning with Python". См. код \href{https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.3-advanced-usage-of-recurrent-neural-networks.ipynb}{здесь}



\begin{center}
\includegraphics[width=10cm]{./pics/mlp_loss.png}
\end{center}

Отсюда видно, что нейросеть выучила всю информацию из данных и строить модель сильнее нет смысла. Например, если
попробовать обучить LSTM на тех же данных, то можно увидеть, что сеть не сможет превзойти результат MLP:
\begin{center}
\includegraphics[width=10cm]{./pics/lstm_loss.png}
\end{center}
\subsection{LSTM}
\label{sec:orge03b6de}
LSTM обучается абсолютно так же, как и MLP
\subsection{SARIMA(facebook prophet)}
\label{sec:org378c071}
Так как наши данные периодичны с периодом в год, то вместо ARIMA, нужно использовать ARIMA с поддержкой сезонности -
SARMIMA. SARIMA - это модель, которая обобщает линейную регрессию, всзвешенное усреднение, диференцирование временнного
ряда, экспоненциальное сглаживание. Это все простые модели, которые можно проверить на наших данных, использовав только
1 модель - SARIMA.

SARIMA делает следующие допущения насчет данных - временной ряд стационарен:
\begin{itemize}
\item нет тренда
\item нет сезонности
\item дисперсия всюду одинакова
\end{itemize}

Проверку всех этих предположений, исправляение нестационарного ряда в стационарный и примененине SARMIMA реализовано в
пакете facebook prophet

\begin{center}
\includegraphics[width=15cm]{./pics/fb_prophet_prediction_1.png}
\end{center}
\begin{center}
\includegraphics[width=15cm,height=10cm]{./pics/fb_prophet_prediction_2.png}
\end{center}

Из графиков видно, что fb prophet настраивается на тренд, но на колебания возле тренда настроится не может
\subsection{XGBoost}
\label{sec:org1f17f3d}
\begin{center}
\includegraphics[width=15cm]{./pics/xgboost_predictions_3_cities.png}
\end{center}


\begin{center}
\includegraphics[width=15cm]{./pics/xgboost_feature_importance_3_cities.png}
\end{center}

После добавления еще 3х городов между Брайтон энд Хов и Лондоном


\begin{center}
\includegraphics[width=15cm]{./pics/dartford_crawley_brancknell.png}
\end{center}


\begin{center}
\includegraphics[width=15cm]{./pics/xgboost_feature_importance_6_cities.png}
\end{center}


\begin{center}
\includegraphics[width=15cm]{./pics/xgboost_predictions_3_cities.png}
\end{center}

\subsection{Результаты экспериментов}
\label{sec:org3b8cbde}
baseline модель предсказывает температуру на следующий час по предыдущему значению, для нее нет смысла в разделении
выборки на обучающую и тренировочную

\begin{center}
\begin{tabular}{lrl}
модель & mae на валидационной выборке, градусы цельсия & mae на обучающей выборке, градусы цельсия\\
\hline
baseline & 2.20 & -\\
MLP & 2.00 & \\
LSTM & 1.99 & \\
XGBoost, 6 городов & 0.18 & 0.176\\
XGBoost, 3 города & 0.46 & 0.427\\
SARIMA(fb prophet) & 9.2 & \\
\hline
\end{tabular}
\end{center}

\section{Ссылки}
\label{sec:org60607c4}
\href{http://www.machinelearning.ru/wiki/images/archive/f/fc/20130211221536\%21Voron-ML-Intro-slides.pdf}{Основные понятия и обозначения в машинном обучении. Воронцов К.В.}
\section{Список литературы}
\label{sec:org5a3ab64}
\bibliographystyle{unsrt}
\bibliography{manuscript,manuscript}
\section{Приложение}
\label{sec:org2e8d6ae}
\subsection{Алгоритм закачки данных с darksky}
\label{sec:org0be4789}
\begin{verbatim}
  cities = get_cities(['Oxford', 'Cambridge', 'Brighton And Hove', 'London'])
  for city, key in zip(cities[: len(keys)], keys.keys()): 
      df = pd.DataFrame()
      date_start = get_last_downloaded_date(city[0])
      date_end = dt(2018, 1, 1, hour=0)
    
      try:
          date_list = get_list_of_days(date_start, date_end)
      except AssertionError:
          continue

      logger.info(f"Скачиваю данные с города {city[0]}")
      for date in tqdm(date_list):
          try:
              _city = forecast(key, city[1], city[2], time=date)
              error = False
          except requests.exceptions.HTTPError as e:
              error = True
              logger.error(str(e.request) + str(e.response) + str(e))
              break
          except Exception as e:
              error = True
              logger.error(str(e))
              break
          try:
              for i in range(len(_city.hourly)):
                  values = [to_date_from_unix_time(_city.hourly[i]['time'])]
                  for column in columns:
                      try:
                          values.append(_city.hourly[i][column])
                      except KeyError as e:
                          values.append(None)
                      t = pd.DataFrame(values).T
                    
                      df = pd.concat((df ,t))
              except AttributeError as e:
                  logger.error(str(e))
                  error =True

          if df.shpape[0] > 0:
              df.columns = ["time"] + columns
              df = df.set_index("time")

              path = os.path.join(CURRENT_DIR, "diplom_data/" + str(city[0]) + ".csv")
              with open(path, 'a') as f:
                  df.to_csv(f, index=True, header=False)
                
                  keys[key] = True #key is used, dont use it again today
\end{verbatim}
\subsection{Код самого первого решения (baseline)}
\label{sec:org0c8cfbe}
\begin{verbatim}
def evaluate_naive_method():
    batch_maes = []
    for step in range(val_steps):
        samples, targets = next(val_gen)
        preds = samples[:, -1, 1]
        mae = np.mean(np.abs(preds - targets))
        batch_maes.append(mae)
    return np.mean(batch_maes)
\end{verbatim}
\subsection{MLP}
\label{sec:org61defcb}
\begin{verbatim}
model = Sequential()
model.add(layers.Flatten(input_shape=(lookback // step, data.shape[-1])))
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(1))

model.compile(optimizer=RMSprop(), loss='mae')
callbacks_list = [
    keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.1,
    patience=1,
    verbose = 1
    )
]
history = model.fit_generator(train_gen,
                              steps_per_epoch=train_steps,
                              epochs=20,
                              validation_data=val_gen,
                              validation_steps=val_steps,
                              callbacks=callbacks_list)
\end{verbatim}
\end{document}
