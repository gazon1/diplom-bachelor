#+TITLE: Предсказание температуры во времени и пространстве
#+DATE: <2019-04-08>
#+AUTHOR: Дробин М.Е. (МФТИ ГУ)
#+EMAIL: drobin.me@phystech.edu


#+EXCLUDE_TAGS: journal noexport
#+latex_header: \usepackage[utf8]{inputenc} % for cyrilics
#+latex_header: \usepackage[russian]{babel}
#+latex_header: \usepackage[T2A]{fontenc}

#+begin_abstract
В этой работе исследуется качество прогнозирования температуры для одной из метерологической станций, обучая нейросетевые алгоритмы(RBF, MLP) и
классические(KNN, spatial averaging, inverse distance methods) по данных от других станций в пределах Англии.
#+end_abstract

* TODOS                                                            :noexport:
** TODO Привести картинку seasonal_decompose

** TODO найти статьи с предсказанием временного ряда(температуры воздуха)[0/3]
- [ ] с моделью LSTM
- [ ] ARIMA
- [ ] XGBoost
** DONE Обучить facebook prophet
CLOSED: [2019-05-29 Ср 16:04]
** DONE Погулить, какие можно картинки выдать с facebook prophet
CLOSED: [2019-05-29 Ср 16:04]

** TODO Посчитать mae на обучающей выборке [0/3]
- [ ] MLP
- [ ] LSTM
- [ ] fb prophet

** TODO Посчиттаь mae для fb prophet на val set
** TODO Перебрать кол-во деревьев в xgboost
https://github.com/maxis42/ML-DA-Coursera-Yandex-MIPT/blob/master/2%20Supervised%20learning/Lectures%20notebooks/11%20xgboost%20gradient%20boosting/sklearn.rf_vs_gb.ipynb
** TODO Погуглить на kaggle, какие параметры перебирают у xgboost
** TODO Переписать теорию xgboost от Евгения Соколова [[https://github.com/maxis42/ML-DA-Coursera-Yandex-MIPT/blob/master/2%2520Supervised%2520learning/Lectures/4-3.Gradientnyj_busting.pdf][link]]
* notes                                                            :noexport:
[[https://www.youtube.com/watch?v=2t925KRBbFc][Introduction to org-ref Jogn Kitchin]]
* Введение
  Температура воздуха - временные ряды с высоким временным разрешением - измеряют только в немногочисленных, далеко
  разнесенных друг от друга метерологических станциях.  Поэтому появляется необходимость в пространственной интерполяции
  этих значений на области, где температура. Кроме пространственной интерполяции появляется необходимость в
  прогнозировании будущей темпертуры воздуха, т.е. временной интерполяции. Такие задачи появляются в сельском хозяйстве,
  прогнозировании погоды в городских условиях и т.д.

  В качестве данных используется почасовая информация(влажность, ...) с 30 метерологических станций Англии(каждая в
  отдельном городе) за 2 года. И для некоторых из них предсказывается температура. Данные взяты из сервиса darksky.




* Постановка задачи
Даны временные ряды $temp_{L}(t)$ - температура воздуха, замеренная на
одной из погодных станций в Лондоне, и $\vec{x_{j}}(t), j \in \{1, 2,
...\}$ - климатические данные городов в окрестности Лондона
(температура, давление, ...). Необходимо предсказать $temp_{L}(t + 1)$

Обобщенная модель для решения этой задачи выглядит следующим образом:
$temp_{L}(t + 1) = f(temp_{L}(t), \vec{x_{1}}(t), \vec{x_{2}}(t),
...)$. Исследуется зависимость f от близости городов в окрестности
Лондона к самому Лондону. Также исследуется качество известных моделей
машинного обучениня примененых к этим данным. Приводится обоснование
выбора функции потерь для этой задачи, а потому, выводы, построенные
по качеству и некоторым характеристикам примененных моделей зависят не
от моделей, а от самих данных.
* Обзор литературы
В статье cite:Spatial-Interpolation-2000-Snell aвторы сравнили
  качество MLP и spatial average, knn, inverse distance methods для
  задачи интерполяции температуры на 11 NOAA станций, на которых
  измеряют температуру. Эти станции расположены примерно в одном
  штате. Температура с этих станций - это множество ответов алгоритма,
  а данные с GCM(general circulation model) - численное модели - это
  мн-возможным объектов. Такая задача называется downscaling
  GCM. Трудность заключается в том, что пространственное разрешение
  выходных данных GCM - от 2.5° × 2.5° до 8° × 10° в долготу и
  ширину - это слишком грубо для предсказания - поэтому обучают по
  такой сетке нейросеть и предсказывают температуру для точек между
  сетки - выхода GCM. Авторы предсказывали максимальную температуру за
  день T_max и посчитали качество нейросети(архите- ктура нейросети:
  4-30-11 и 16-54-11 с сигмоидной функцией активацией). Для сетки с
  4мя входами R^2 и rmse были в среднем по 11 станциям 5.69 и 0.93;
  для сетки с 16ю входами - 5.12 и 0.94.

  В статье cite:nn-prediction-hourly-mean-temp-Tasadduq-2002 авторы
  использовали почасовые данные с шести метерологических станций за 2
  года с равнины, расположенной в Китае и ограниченной горами. 5 из
  них входят в мн-во объектов алгоритмов, для шестой предсказывается
  температу.  60% данных использовалось для обучения, остальное для
  теста. Сравнили MLP 5-17-1 с tanh активацией и RBF с скрытым слоем
  из 7450 нейронов и гауссовой функцией активацией. Качество на
  тестовой выборке: $R^2$ и rmse(°C) для MLP равны 0.96 и 1.067, а для
  RBF - 0.95 и 1.12 соответственно.

  Данные по нескольким городам - погодные данные, взятые из измерителей в аэропортах этих городов.
Задача - предсказать температуру(или прочие погодные показатели, напр., влажность..) для города, не использованного
в обучении,  используя нейросетевые алгоритмы.






* Получение данных
Данные были получены через API сервиса [[https://darksky.net][darksky]] и библиотеки для Python [[https://pypi.org/project/darkskylib/][darkskylib]]. Для каждого города были скачаны
почасовые данные(иногда с пропусками) с с 1 января 2009 по 1 января 2018 года. Следующие фичи были скачаны:
- температура воздуха(temperature)
- скорость ветра (windspeed)
- процент(от 0 до 1) покрытости неба облаками(cloudCover)
- относительная влажность, от 0 до 1 (humidity)
- видимость(visibility) - категориальная переменная, принимает только
- точка росы в градусах цельсия(dewPoint)
- краткое описание погоды - категориальная переменная (summary)
  - clear-day
  - clear-night
  - rain
  - snow
  - sleet
  - wind
  - fog
  - cloudy
  - partly-cloudy-day
  - partly-cloudy-night
- температура "по ощущениям" в градусах цельсия (apparentTemperature)

[[https://github.com/gazon1/diplom/blob/master/main.py][Алгоритм]] закачки данных:
for каждый город в списке городов:
    for каждая дата в списке дат с 2009 по 2018 год c периодом в  1 час:
        скачать исторические данные за эту дату для этого города
    добавить данные за этот город в общий датафрейм

* EDA
#+ATTR_LATEX: :width 15cm 
[[file:./pics/sesonal_decomposition.png]]
[[file:./pics/rolling_mean.png]]

- Есть ненулевой тренд
- четко выраженная дневная сезонность
- годовая сезонность
* Эксперименты

** Схема валидации
Модели обучались на первых 80% данных - до 2016-03-15. Валидировались модели на оставшихся 20% данных - около 2х
лет. Причина выбора такой схемы валидации проста -  у нас имеется относительно большое кол-во данных(в сравнии с чем?) и
более сложные схемы валидации, например, [[https://habr.com/ru/company/ods/blog/327242/][cross-validation on a rolling basis]], оказываются не нужны для построения
устойчивой оценки алгоритма. Более сложные схемы валидации часто применяют, когда данных мало и делить исходную выборку
на 2 невыгодно.

** Baseline
В качестве алгоритма для сравнения было взято простое предсказание температуры, равное предыдущему значению:
*** TODO написать в математической формулировке с формула как я предсказываю в baseline

Этот алгоритм предсказывает на валидационной выборке с точностью до +/- 2.2 градуса Цельсия

** MLP
Модель обучались на первых 80% данных - до 2016-03-15. Валидировалась - на оставшихся 20% данных - около 2х лет.

Данные были нормированы на среднее и дисперсию: $x_{i} = \frac{x_{i} - \overline{x}}{\sigma}$, где x - это отдельная
фича или таргет (колонка в массиве объекты x фичи) и берется дисперсия и среднее этой фичи и она нормирутеся на свое
среднее и свою дисперсию

фичи: только температуры 3 городов Оксфорд, Кембридж, Брайтон энд Хов, таргет - это Лондон.

#+ATTR_LATEX: :width 15cm 
[[file:./pics/map city predict.png]] 

MLP обучался следующим образом: брали данные за 5 дней и температуру Лондона на следующие 24 часа. Оптимизировали mae.
Архитектура нейросети: полносвязный слой с 32 нейронами и relu активацией и полносвязный слой с одним нейроном без
функции активации на выходе. Оптимизатор - RMSprop. Для более быстрой и лучшей сходимости, скорость обучения делилась
на 10, когда функция потерь на валидации увеличивалась или не изменялась:

Генератор данных на керасе для обучения нейросети был заимствован из книги "Deep learning with Python". См. код [[https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.3-advanced-usage-of-recurrent-neural-networks.ipynb][здесь]]



#+ATTR_LATEX: :width 10cm 
[[file:./pics/mlp_loss.png]]

Отсюда видно, что нейросеть выучила всю информацию из данных и строить модель сильнее нет смысла. Например, если
попробовать обучить LSTM на тех же данных, то можно увидеть, что сеть не сможет превзойти результат MLP:
#+ATTR_LATEX: :width 10cm 
[[file:./pics/lstm_loss.png]]
** LSTM
LSTM обучается абсолютно так же, как и MLP
** SARIMA(facebook prophet)
Так как наши данные периодичны с периодом в год, то вместо ARIMA, нужно использовать ARIMA с поддержкой сезонности -
SARMIMA. SARIMA - это модель, которая обобщает линейную регрессию, всзвешенное усреднение, диференцирование временнного
ряда, экспоненциальное сглаживание. Это все простые модели, которые можно проверить на наших данных, использовав только
1 модель - SARIMA.

SARIMA делает следующие допущения насчет данных - временной ряд стационарен:
  - нет тренда
  - нет сезонности
  - дисперсия всюду одинакова

Проверку всех этих предположений, исправляение нестационарного ряда в стационарный и примененине SARMIMA реализовано в
пакете facebook prophet

#+ATTR_LATEX: :width 15cm 
[[file:./pics/fb_prophet_prediction_1.png]]
#+ATTR_LATEX: :width 15cm :height 10cm
file:./pics/fb_prophet_prediction_2.png

Из графиков видно, что fb prophet настраивается на тренд, но на колебания возле тренда настроится не может
** XGBoost
#+ATTR_LATEX: :width 15cm 
[[file:./pics/xgboost_predictions_3_cities.png]]


#+ATTR_LATEX: :width 15cm 
[[file:./pics/xgboost_feature_importance_3_cities.png]]

После добавления еще 3х городов между Брайтон энд Хов и Лондоном


#+ATTR_LATEX: :width 15cm 
[[file:./pics/dartford_crawley_brancknell.png]]


#+ATTR_LATEX: :width 15cm 
[[file:./pics/xgboost_feature_importance_6_cities.png]]


#+ATTR_LATEX: :width 15cm 
[[file:./pics/xgboost_predictions_3_cities.png]]

** Результаты экспериментов
baseline модель предсказывает температуру на следующий час по предыдущему значению, для нее нет смысла в разделении
выборки на обучающую и тренировочную

| модель             | mae на валидационной выборке, градусы цельсия | mae на обучающей выборке, градусы цельсия |
|--------------------+-----------------------------------------------+-------------------------------------------|
| baseline           |                                          2.20 |                                         - |
| MLP                |                                          2.00 |                                           |
| LSTM               |                                          1.99 |                                           |
| XGBoost, 6 городов |                                          0.18 |                                     0.176 |
| XGBoost, 3 города  |                                          0.46 |                                     0.427 |
| SARIMA(fb prophet) |                                           9.2 |                                           |
|--------------------+-----------------------------------------------+-------------------------------------------|

* Ссылки
[[http://www.machinelearning.ru/wiki/images/archive/f/fc/20130211221536%2521Voron-ML-Intro-slides.pdf][Основные понятия и обозначения в машинном обучении. Воронцов К.В.]]
* Список литературы
bibliographystyle:unsrt
bibliography:manuscript.bib,~/Yandex.Disk/inbox/diplom/manuscript.bib
* Приложение
** Алгоритм закачки данных с darksky
#+BEGIN_SRC python
  cities = get_cities(['Oxford', 'Cambridge', 'Brighton And Hove', 'London'])
  for city, key in zip(cities[: len(keys)], keys.keys()): 
      df = pd.DataFrame()
      date_start = get_last_downloaded_date(city[0])
      date_end = dt(2018, 1, 1, hour=0)
    
      try:
          date_list = get_list_of_days(date_start, date_end)
      except AssertionError:
          continue

      logger.info(f"Скачиваю данные с города {city[0]}")
      for date in tqdm(date_list):
          try:
              _city = forecast(key, city[1], city[2], time=date)
              error = False
          except requests.exceptions.HTTPError as e:
              error = True
              logger.error(str(e.request) + str(e.response) + str(e))
              break
          except Exception as e:
              error = True
              logger.error(str(e))
              break
          try:
              for i in range(len(_city.hourly)):
                  values = [to_date_from_unix_time(_city.hourly[i]['time'])]
                  for column in columns:
                      try:
                          values.append(_city.hourly[i][column])
                      except KeyError as e:
                          values.append(None)
                      t = pd.DataFrame(values).T
                    
                      df = pd.concat((df ,t))
              except AttributeError as e:
                  logger.error(str(e))
                  error =True

          if df.shpape[0] > 0:
              df.columns = ["time"] + columns
              df = df.set_index("time")

              path = os.path.join(CURRENT_DIR, "diplom_data/" + str(city[0]) + ".csv")
              with open(path, 'a') as f:
                  df.to_csv(f, index=True, header=False)
                
                  keys[key] = True #key is used, dont use it again today
#+END_SRC
** Код самого первого решения (baseline)
#+BEGIN_SRC python
def evaluate_naive_method():
    batch_maes = []
    for step in range(val_steps):
        samples, targets = next(val_gen)
        preds = samples[:, -1, 1]
        mae = np.mean(np.abs(preds - targets))
        batch_maes.append(mae)
    return np.mean(batch_maes)
#+END_SRC
** MLP
#+BEGIN_SRC python
model = Sequential()
model.add(layers.Flatten(input_shape=(lookback // step, data.shape[-1])))
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(1))

model.compile(optimizer=RMSprop(), loss='mae')
callbacks_list = [
    keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.1,
    patience=1,
    verbose = 1
    )
]
history = model.fit_generator(train_gen,
                              steps_per_epoch=train_steps,
                              epochs=20,
                              validation_data=val_gen,
                              validation_steps=val_steps,
                              callbacks=callbacks_list)
#+END_SRC
